# Predicting Media Memorability

This project aimed to maximise the accuracy of short-term media memorability predictions on the MediaEval Predicting Media Memorability task. The task's dataset is comprised of 10,000 short (soundless) video clips, the text captions which have been produced to annotate the clips, "a set of pre-extracted features, such as: Dense SIFT, HoG descriptors, LBP, GIST, Color Histogram, MFCC, Fc7 layer from AlexNet, C3D features, etc.", as well as the short and long-term memorabilty scores for each clip. More information on the dataset and the experiments performed to collect the memorability scores can be found on the [MediaEval website](http://www.multimediaeval.org/mediaeval2019/memorability/). While ensembled methods produced the best results of the 2019 cohort (see the inspirational work of the researchers at the DCU Insight Centre [here](https://github.com/dazcona/memorability)), due to time constraints (working full time as a Software Developer while working on this project and another project for my Data Mining & Data Analysis course) I decided to solely focus on maximising the short-term memorability scores from captions. This was intended as the development of an additional model which could potentially be implemented with DCU's ensemble at a later stage.

The project was completed as the main assignment of the **CA684 Machine Learning** module of the **MSc. in Computing (Artificial Intelligence)** at Dublin City University, Ireland (a 2 year part-time, online masters). For this assignment students were provided with 80% of the dataset provided by MediaEval and were tasked with building a Machine Learning model from scratch to predict both the short and long-term memorability variables of the remaining 20%. With my caption-based model I achieved some of the best results in the module, earning a mark of **85.7%** on the assignment and coming **first in my class** (of 27 pupils). This module was also shared with 10 other DCU courses (at the PhD level: CAPD, CAPT, EEPT, MEPT; at the MSc level: CAPM and other streams of the MCM course; GCAI Grad Cert in AI; GTEC Grad Training Visitor Program; and the Engineering & Computing Study Abroad courses ECSA & ECSAO). Out of the total **175 students** who completed this assignment I came ***2nd overall***.


## Approach
This approach involved intensive experiments whereby the ML model was trained using multiple permutations of word embeddings in the first layer of the model and text pre-processing techniques. A processor-intensive hyperparameter tuning experiment was performed on the final model to find the optimal kernel regularization and dropout parameters for the model. As the CA684 assignments were performed on Google Colab a Google Cloud Platform VM was created to tackle the processing. Details of how to set up the VM in Colab are shown in the ***Model & Training Functions*** section of the main Jupyter notebook, as shown in the following excerpt:

"
The model training was performed on Google Cloud Platform (GCP) with code modified for this environment (see the notebook annotated with 'GCP' for the code used). **Warning: the following experiments and training procedures will run very slowly on Google Colab without port-forwarding to a VM**. It is highly recommended to set up a VM and run these scripts by port-forwarding to the VM. Details on how to achieve this below.

The method used to run these scripts using a GCP Cloud Compute VM instance in Colab was:



1.   A Deep Learning VM instance with a 16Gb HBM2 NVidia TeslaP100 GPU and 16 CPU cores at 104Gb RAM was created. **Please note:** if using GCP, GPU quotas are placed on GCP accounts. You may need to request a quota increase which could take 48 hours or more to process.The Deep Learning VM option in GCP is recommended and can be deployed with most necessary packages, such as pandas, numpy, TensorFlow, and Jupyter, already installed. If other setups are used please ensure that the necessary packages are installed. Be aware that some troubleshooting may be needed to run the notebook.
2.   The local machine being used to run Colab was port-forwarded to this VM by running the command `gcloud compute ssh $VM_INSTANCE --zone $VM_ZONE -- -L 8080:localhost:8080`, where VM_INSTANCE is the name of assigned to the VM and VM_ZONE is the name of the zone the VM was assigned to during creation.
3. With port 8080 of your computer connected to the VM (which can be confirmed by checking http://localhost:8080/ in your browser) you can now connect Colab to it by selecting the dropdown next to "Connect" in the top right of the Colab window and selecting "Connect to local runtime". In here place http://localhost:8080/ in the "Backend URL" field, replacing "8080" with the port you used when port-forwarding.
4. Using the notebook titled "GCP_Upload", connect using the process above and run the script in the notebook, replacing the `project_id` and `bucket_name` variables with your own project and bucket info. If you have not yet created a project or bucket you can find instructions for this easily online or within GCP, such as [here](https://cloud.google.com/storage/docs/creating-buckets).
5. With the processed captions uploaded to your bucket, you can now run the experiments in the notebook "CA684_Assignment-FinalGCP". Be sure to replace bucket and project references within this file also.
6. Once the experiments and training is completed you can download the files directly from the VM or transfer from the VM's local files to a GCloud bucket using the command `gsutil -m cp -r dir gs://my-bucket` (found in the GCP notebook).
7. Files can be retrieved from your GCP bucket to your local machine using the command `gsutil -m cp -R gs://<your-bucket-name>/<folder-name> .` in a terminal window/command prompt on your device. To retrieve all files from the bucket simply pass only the bucket name and do not include a folder. The `-R` argument will ensure the copy is recursive, collecting all files and subdirectories, while `-m` makes use of multithreading to speed up the process. More info on gsutil commands [here](https://cloud.google.com/storage/docs/gsutil/commands/cp).
"

Due to the enormous amount of data produced by the experiments and project I have excluded much of it from the Git repository, such as the dataset, roughwork, experminent results and hyperparameter tuning data etc. I will happily share this upon request.